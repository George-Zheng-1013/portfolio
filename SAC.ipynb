{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzJJSo05CVmX"
      },
      "source": [
        "# Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 数据准备"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tech_daily = pd.read_csv(r\"data\\科技股票.csv\")\n",
        "tech_daily.set_index('date', inplace=True)\n",
        "tech_daily.columns=['AAPL','GOOG','MSFT']\n",
        "tech_daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debt=pd.read_csv(r\"data\\无风险.csv\",encoding='gbk')\n",
        "debt.set_index('date', inplace=True)\n",
        "debt.columns=['US_debt']\n",
        "debt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp=pd.read_csv(r\"data\\指数和贵金属.csv\", encoding='gbk')\n",
        "tmp.columns=['date','SP500','Gold']\n",
        "tmp.set_index('date', inplace=True)\n",
        "tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.merge(tech_daily,debt,how='left',on='date')\n",
        "df=pd.merge(df,tmp,how='left',on='date')\n",
        "df['date']=pd.to_datetime(df.index)\n",
        "df.set_index('date', inplace=True)\n",
        "df[df.isnull().values == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.interpolate(method='time', inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNqvPxkgCEc5"
      },
      "outputs": [],
      "source": [
        "from math import inf\n",
        "\n",
        "from networkx import sigma\n",
        "from pyparsing import deque\n",
        "\n",
        "class PortfolioOptimizationEnv(gym.Env):\n",
        "    def __init__(self, tickers, window_size, start_date, end_date,\n",
        "                 initial_balance, seed=None):\n",
        "        super().__init__()\n",
        "        self.tickers = tickers\n",
        "        self.window_size = window_size\n",
        "        self.initial_balance = initial_balance\n",
        "\n",
        "        # 分别存储原始价格和指标\n",
        "        self.raw_data, self.feature_data = self.get_data(tickers, start_date, end_date)\n",
        "        self.n_features = self.feature_data.shape[1]\n",
        "\n",
        "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(len(tickers),))\n",
        "        self.observation_space = gym.spaces.Box(low=-inf, high=inf,\n",
        "                                        shape=(window_size, self.n_features))\n",
        "\n",
        "        self.return_window=deque(maxlen=window_size)\n",
        "        self.last_action=np.ones(len(tickers))/len(tickers)\n",
        "\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "            self.action_space.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    def get_data(self, tickers, start_date, end_date):\n",
        "        data = df.copy().dropna()\n",
        "        data = data.loc[start_date:end_date, tickers]\n",
        "\n",
        "        # 保存原始价格（用于计算投资组合收益）\n",
        "        raw_data = data.copy()\n",
        "\n",
        "        # 计算特征指标\n",
        "        returns = data.pct_change()\n",
        "\n",
        "        mom_frames = []\n",
        "        for window in [5, 20]:\n",
        "            mom = data / data.shift(window) - 1\n",
        "            mom.columns = [f\"{col}_mom_{window}\" for col in data.columns]\n",
        "            mom_frames.append(mom)\n",
        "\n",
        "        vol = returns.rolling(window=20, min_periods=1).std()\n",
        "        vol.columns = [f\"{col}_vol_20\" for col in data.columns]\n",
        "\n",
        "        ma = data.rolling(window=20, min_periods=1).mean()\n",
        "        ma_dev = data / ma - 1\n",
        "        ma_dev.columns = [f\"{col}_ma_dev_20\" for col in data.columns]\n",
        "\n",
        "        returns.columns = [f\"{col}_ret\" for col in data.columns]\n",
        "\n",
        "        # 特征数据：returns, vol, ma_dev, momentum（不包含原始价格）\n",
        "        feature_data = pd.concat([returns, vol, ma_dev] + mom_frames, axis=1)\n",
        "        raw_data = raw_data.dropna()\n",
        "        feature_data = feature_data.reindex(raw_data.index)\n",
        "        feature_data.fillna(method='ffill', inplace=True)\n",
        "        feature_data.fillna(method='bfill', inplace=True)\n",
        "\n",
        "        return raw_data.dropna(), feature_data.dropna()\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self.balance = self.initial_balance\n",
        "        self.current_step = self.window_size\n",
        "\n",
        "        self.return_window.clear()\n",
        "        self.last_action = np.ones(len(self.tickers)) / len(self.tickers)\n",
        "\n",
        "        # 使用特征数据作为观察\n",
        "        obs = self.feature_data.iloc[self.current_step - self.window_size:self.current_step].values\n",
        "        info = {\"balance\": self.balance}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        # SAC已经输出归一化的动作，因此不需要进行softmax\n",
        "        action=np.asarray(action).ravel()\n",
        "        action = np.clip(action, 0, 1)\n",
        "        action = action / np.sum(action+1e-8)\n",
        "\n",
        "        prev_balance = self.balance\n",
        "\n",
        "        # 从原始价格计算实际收益\n",
        "        current_price = self.raw_data.iloc[self.current_step].values[:len(self.tickers)]\n",
        "        prev_price = self.raw_data.iloc[self.current_step - 1].values[:len(self.tickers)]\n",
        "        asset_returns = current_price / prev_price - 1\n",
        "\n",
        "        self.return_window.append(asset_returns)\n",
        "\n",
        "        # 基础奖励：投资组合收益\n",
        "        portfolio_return = np.sum(asset_returns * action)\n",
        "        self.balance = self.balance * (1 + portfolio_return)\n",
        "        base_reward = np.log(self.balance / prev_balance)\n",
        "\n",
        "        risk_penalty = 0\n",
        "        if len(self.return_window)>=5:\n",
        "            R=np.vstack(self.return_window)\n",
        "            cov_matrix=np.cov(R.T)\n",
        "            sigma_p2= action.T @ cov_matrix @ action\n",
        "            sigma_p=np.sqrt(sigma_p2)\n",
        "            lambda_vol=5\n",
        "            risk_penalty = -lambda_vol * sigma_p\n",
        "\n",
        "        turnover=np.sum(np.abs(action - self.last_action))\n",
        "        cost= 0.001 * turnover\n",
        "        self.last_action=action\n",
        "\n",
        "        # 总奖励\n",
        "        reward = base_reward + risk_penalty - cost\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.raw_data)-1\n",
        "\n",
        "        obs_end=min(len(self.feature_data),self.current_step+self.window_size)\n",
        "        obs_start=max(0,obs_end - self.window_size)\n",
        "        obs = self.feature_data.iloc[obs_start:obs_end].values\n",
        "\n",
        "        terminated = bool(done)\n",
        "        truncated = False\n",
        "        info = {'balance': self.balance}\n",
        "\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFJ-OZPqLeE9",
        "outputId": "691d4617-57cf-40bc-a093-00a61ea23f23"
      },
      "outputs": [],
      "source": [
        "tickers = df.columns.tolist()\n",
        "window_size = 30\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2025-09-01'\n",
        "initial_balance = 10000\n",
        "seed = 8\n",
        "\n",
        "# Initialize the environment\n",
        "env = PortfolioOptimizationEnv(\n",
        "    tickers,\n",
        "    window_size,\n",
        "    start_date,\n",
        "    end_date,\n",
        "    initial_balance,\n",
        "    seed)\n",
        "\n",
        "# Get the initial state\n",
        "state = env.reset(seed=seed)\n",
        "# Sample and execute a random action\n",
        "action = env.action_space.sample()\n",
        "next_state, reward, terminated, truncated, info = env.step(action)\n",
        "done = bool(terminated or truncated)\n",
        "# print(f\"State: {state}\")\n",
        "print(f\"Action: {action}\")\n",
        "# print(f\"Next state: {next_state}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Balance: {info['balance']}\")\n",
        "print(f\"Done: {done}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcJV_sJQ8wGl"
      },
      "source": [
        "# Training DRL agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc9hmosIN5da",
        "outputId": "a2697902-fd1f-4a8b-bf33-bd764de0c06c"
      },
      "outputs": [],
      "source": [
        "# ===== Train SAC + plot training profile + plot cumulative wealth + print metrics =====\n",
        "# Prereqs (run once in your env if needed):\n",
        "#   pip install \"stable-baselines3>=2.3.0\" \"shimmy>=2.0\" matplotlib pandas\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "# ------------ 1) Build vectorized env (wrap with Monitor to log training rewards) ------------\n",
        "log_dir = \"./sb3_logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "monitor_path = os.path.join(log_dir, \"monitor.csv\")\n",
        "\n",
        "def make_env():\n",
        "    env_ = PortfolioOptimizationEnv(\n",
        "        tickers=tickers,\n",
        "        window_size=window_size,\n",
        "        start_date=start_date,\n",
        "        end_date=end_date,\n",
        "        initial_balance=initial_balance,\n",
        "        seed=seed,\n",
        "    )\n",
        "    # Log episode reward/length; include \"balance\" from info if you want (optional)\n",
        "    env_ = Monitor(env_, filename=monitor_path)  # single-env -> single monitor file\n",
        "    return env_\n",
        "\n",
        "vec_env = DummyVecEnv([make_env])\n",
        "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
        "\n",
        "# ------------ 2) Train SAC ------------\n",
        "model = SAC(\n",
        "    \"MlpPolicy\",\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    device=device,\n",
        "    learning_rate=3e-4,\n",
        "    batch_size=256,\n",
        "    tau=0.005,\n",
        "    buffer_size=1_000_000,\n",
        "    ent_coef=\"auto\",\n",
        "    train_freq=(1, \"step\"),\n",
        "    gradient_steps=1,\n",
        ")\n",
        "model.learn(total_timesteps=20000)  # increase as needed\n",
        "model.save(\"sac_portfolio_optimization\")\n",
        "\n",
        "# ------------ 3) Load training profile (episode rewards) and plot ------------\n",
        "# SB3 Monitor CSV starts with commented metadata lines beginning with '#'\n",
        "train_df = pd.read_csv(monitor_path, comment=\"#\")\n",
        "# Columns typically: r (ep reward), l (ep length), t (time)\n",
        "# Make a simple moving average of episode rewards for a smooth training curve\n",
        "if len(train_df) > 0:\n",
        "    train_df[\"ep\"] = np.arange(1, len(train_df) + 1)\n",
        "    train_df[\"reward_smooth\"] = train_df[\"r\"].rolling(window=max(5, len(train_df)//50), min_periods=1).mean()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_df[\"ep\"], train_df[\"r\"], alpha=0.3, label=\"Episode reward\")\n",
        "    plt.plot(train_df[\"ep\"], train_df[\"reward_smooth\"], label=\"Smoothed\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode reward\")\n",
        "    plt.title(\"Training profile (episode rewards)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No episodes logged in monitor file (did training terminate too early?).\")\n",
        "\n",
        "# ------------ 4) Roll out a full episode deterministically and record balances ------------\n",
        "obs = vec_env.reset()\n",
        "dones = [False]\n",
        "balances = []\n",
        "rewards = []\n",
        "weights_seq = []\n",
        "\n",
        "while not dones[0]:\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, dones, infos = vec_env.step(action)\n",
        "    rewards.append(float(reward[0]))\n",
        "    balances.append(float(infos[0].get(\"balance\", np.nan)))\n",
        "    weights_seq.append(infos[0].get(\"weights\", None))\n",
        "\n",
        "# Try to build a date index for nicer plotting/annualization\n",
        "try:\n",
        "    # Access the underlying single env inside DummyVecEnv -> Monitor -> PortfolioOptimizationEnv\n",
        "    base_env = vec_env.envs[0].env\n",
        "    # For safety if nested wrappers differ:\n",
        "    while hasattr(base_env, \"env\"):\n",
        "        base_env = base_env.env\n",
        "    price_index = base_env.data.index  # pandas.DatetimeIndex from your env\n",
        "    # The first balance corresponds to the first step after initial window\n",
        "    # Align dates accordingly:\n",
        "    start_idx = window_size\n",
        "    end_idx = start_idx + len(balances)\n",
        "    dates = price_index[start_idx:end_idx]\n",
        "except Exception:\n",
        "    dates = pd.RangeIndex(start=1, stop=len(balances) + 1)  # fallback to simple index\n",
        "\n",
        "# ------------ 5) Plot cumulative wealth curve ------------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(dates, balances)\n",
        "plt.xlabel(\"Date\" if isinstance(dates, pd.DatetimeIndex) else \"Step\")\n",
        "plt.ylabel(\"Wealth\")\n",
        "plt.title(\"Cumulative wealth (deterministic policy rollout)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------ 6) Compute portfolio performance metrics ------------\n",
        "# Step simple returns from balance path\n",
        "balances_arr = np.asarray(balances, dtype=float)\n",
        "rets = balances_arr[1:] / balances_arr[:-1] - 1.0 if len(balances_arr) > 1 else np.array([])\n",
        "\n",
        "# Infer periods per year from dates if possible; else default 252\n",
        "def infer_ppy(idx):\n",
        "    if isinstance(idx, pd.DatetimeIndex) and len(idx) >= 2:\n",
        "        # Use median days between points\n",
        "        deltas = np.diff(idx.view(\"int64\")) / 1e9 / 86400.0  # ns -> days\n",
        "        median_days = np.median(deltas) if len(deltas) else 1.0\n",
        "        if median_days <= 0:\n",
        "            return 252.0\n",
        "        return 365.25 / median_days\n",
        "    return 252.0\n",
        "\n",
        "PPY = infer_ppy(dates)\n",
        "\n",
        "def max_drawdown(equity):\n",
        "    peak = np.maximum.accumulate(equity)\n",
        "    dd = (equity - peak) / peak\n",
        "    return float(dd.min()) if len(dd) else 0.0\n",
        "\n",
        "if len(rets) > 0:\n",
        "    total_growth = balances_arr[-1] / balances_arr[0]\n",
        "    N = len(rets)\n",
        "    CAGR = total_growth ** (PPY / N) - 1.0 # PPY：每年交易天数；CAGR：年复合增长率\n",
        "    mean_r = float(np.mean(rets))\n",
        "    std_r = float(np.std(rets, ddof=1)) if N > 1 else 0.0\n",
        "    ann_return = (1.0 + mean_r) ** PPY - 1.0 if mean_r > -1 else np.nan  # geometric approx alternative below\n",
        "    ann_vol = std_r * np.sqrt(PPY) if std_r > 0 else 0.0\n",
        "    # Sharpe (rf=0 for simplicity; replace with your risk-free rate if desired)\n",
        "    rf = 0.0\n",
        "    sharpe = (mean_r - rf/PPY) / std_r * np.sqrt(PPY) if std_r > 0 else np.nan\n",
        "    mdd = max_drawdown(balances_arr)\n",
        "    calmar = (CAGR / abs(mdd)) if mdd != 0 else np.nan\n",
        "\n",
        "    # Alternative geometric annualized return from total growth (more stable):\n",
        "    ann_return_geom = total_growth ** (PPY / N) - 1.0\n",
        "\n",
        "    print(\"\\n=== Portfolio Performance (deterministic rollout) ===\")\n",
        "    print(f\"Periods per year (inferred): {PPY:.2f}\")\n",
        "    print(f\"Start wealth:  {balances_arr[0]:.2f}\")\n",
        "    print(f\"End wealth:    {balances_arr[-1]:.2f}\")\n",
        "    print(f\"Total growth:  {total_growth:.6f}\")\n",
        "    print(f\"CAGR:          {CAGR:.6%}\")\n",
        "    print(f\"Ann. Return (geom from equity): {ann_return_geom:.6%}\")\n",
        "    print(f\"Ann. Vol:      {ann_vol:.6%}\")\n",
        "    print(f\"Sharpe (rf=0): {sharpe:.4f}\")\n",
        "    print(f\"Max Drawdown:  {mdd:.2%}\")\n",
        "    print(f\"Calmar:        {calmar:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough steps to compute metrics (need at least 2 balances).\")\n",
        "\n",
        "# ------------ 7) (Optional) Inspect last action weights ------------\n",
        "if any(w is not None for w in weights_seq):\n",
        "    last_w = [w for w in weights_seq if w is not None][-1]\n",
        "    print(\"\\nLast portfolio weights:\", np.round(last_w, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 修改部分"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "添加了科技股票、国债、标普500和黄金等数据作为环境输入特征"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 收益严重偏大，进行修改："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "=== Portfolio Performance (deterministic rollout) ===\n",
        "Periods per year (inferred): 252.00\n",
        "Start wealth:  10297.47\n",
        "End wealth:    23731074.08\n",
        "Total growth:  2304.553868\n",
        "CAGR:          804.485651%\n",
        "Ann. Return (geom from equity): 804.485651%\n",
        "Ann. Vol:      22.710210%\n",
        "Sharpe (rf=0): 9.8502\n",
        "Max Drawdown:  -5.59%\n",
        "Calmar:        143.9561"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1、对vec_env进行了归一化处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "=== Portfolio Performance (deterministic rollout) ===\n",
        "Periods per year (inferred): 252.00\n",
        "Start wealth:  10180.25\n",
        "End wealth:    1013360.39\n",
        "Total growth:  99.541782\n",
        "CAGR:          270.067928%\n",
        "Ann. Return (geom from equity): 270.067928%\n",
        "Ann. Vol:      22.074986%\n",
        "Sharpe (rf=0): 6.0514\n",
        "Max Drawdown:  -8.50%\n",
        "Calmar:        31.7818"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2、调整奖励函数\n",
        "用协方差矩阵估计组合波动\n",
        "添加每次交易成本千一"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "=== Portfolio Performance (deterministic rollout) ===\n",
        "Periods per year (inferred): 252.00\n",
        "Start wealth:  10055.45\n",
        "End wealth:    54941.09\n",
        "Total growth:  5.463811\n",
        "CAGR:          62.092061%\n",
        "Ann. Return (geom from equity): 62.092061%\n",
        "Ann. Vol:      17.712724%\n",
        "Sharpe (rf=0): 2.8174\n",
        "Max Drawdown:  -11.18%\n",
        "Calmar:        5.5563"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3、更换为sac算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "删除了step函数中对动作的softmax处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "=== Portfolio Performance (deterministic rollout) ===\n",
        "Periods per year (inferred): 252.00\n",
        "Start wealth:  10145.52\n",
        "End wealth:    22875.43\n",
        "Total growth:  2.254731\n",
        "CAGR:          26.016893%\n",
        "Ann. Return (geom from equity): 26.016893%\n",
        "Ann. Vol:      14.523819%\n",
        "Sharpe (rf=0): 1.6653\n",
        "Max Drawdown:  -12.48%\n",
        "Calmar:        2.0840"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
